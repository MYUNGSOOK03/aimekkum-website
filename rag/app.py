import json
from pathlib import Path

import streamlit as st
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
INDEX_PATH = BASE_DIR / "index" / "faiss.index"
META_PATH = BASE_DIR / "index" / "metadata.json"
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

@st.cache_resource(show_spinner=False)
def load_index():
    if not INDEX_PATH.exists() or not META_PATH.exists():
        return None, None, None
    index = faiss.read_index(str(INDEX_PATH))
    with open(META_PATH, "r", encoding="utf-8") as f:
        meta = json.load(f)
    model = SentenceTransformer(EMBED_MODEL)
    return index, meta, model


def main():
    st.set_page_config(page_title="ë³´í—˜ ì•½ê´€ ì±—ë´‡ (ë¡œì»¬)", page_icon="ğŸ¤–", layout="wide")

    st.title("ğŸ¤– ë³´í—˜ ì•½ê´€ ì±—ë´‡")
    st.caption("PDF ì˜¬ë¦¬ê³  ì§ˆë¬¸í•˜ë©´ ê´€ë ¨ ì¡°í•­ê³¼ í•¨ê»˜ ë‹µë³€í•©ë‹ˆë‹¤ (ë¡œì»¬ RAG ë°ëª¨)")

    with st.expander("1) ì•½ê´€ PDF ì˜¬ë¦¬ê¸° / êµì²´í•˜ê¸°", expanded=True):
        st.write("- ì—¬ê¸°ì—ì„œ PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ `rag/data/` í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.")
        uploaded = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"], accept_multiple_files=True)
        saved_files = []
        if uploaded:
            DATA_DIR.mkdir(parents=True, exist_ok=True)
            for f in uploaded:
                dest = DATA_DIR / f.name
                dest.write_bytes(f.getbuffer())
                saved_files.append(dest.name)
            st.success(f"{len(saved_files)}ê°œ íŒŒì¼ ì €ì¥: {', '.join(saved_files[:3])}{' â€¦' if len(saved_files) > 3 else ''}")

        if st.button("ì¸ë±ìŠ¤ ë‹¤ì‹œ ë§Œë“¤ê¸°"):
            import subprocess, sys
            cmd = [sys.executable, str(BASE_DIR / "build_index.py")]
            with st.spinner("ì¸ë±ìŠ¤ ìƒì„± ì¤‘â€¦"):
                proc = subprocess.run(cmd, capture_output=True, text=True)
            st.code(proc.stdout or "(no stdout)")
            if proc.stderr:
                st.error(proc.stderr)
            # ìºì‹œëœ ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
            st.cache_resource.clear()

    index, meta, model = load_index()
    if index is None:
        st.warning("ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ë„£ê³  'ì¸ë±ìŠ¤ ë‹¤ì‹œ ë§Œë“¤ê¸°'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        return

    query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ) ì…ì›ë¹„ ì²­êµ¬ ì‹œ í•„ìš”í•œ ì„œë¥˜ëŠ”?")
    top_k = st.slider("ì°¸ê³ í•  ì¡°ê° ìˆ˜ (Top-K)", 1, 8, 4)

    if query:
        q_emb = model.encode([query], normalize_embeddings=True)
        q_emb = np.array(q_emb).astype("float32")
        D, I = index.search(q_emb, top_k)
        hits = I[0].tolist()

        st.subheader("ğŸ” ê·¼ê±° ì¡°ê°")
        for rank, idx in enumerate(hits, start=1):
            chunk = meta["texts"][idx]
            info = meta["metadatas"][idx]
            with st.expander(f"#{rank} {info['source']} (chunk {info['chunk_id']})", expanded=rank==1):
                st.write(chunk)

        # ê°„ë‹¨í•œ ê·œì¹™/ìš”ì•½ ê¸°ë°˜ ë‹µì•ˆ (LLM ì—†ì´)
        st.subheader("ğŸ“ ìš”ì•½ ë‹µë³€")
        st.write("ì•„ë˜ëŠ” ìœ ì‚¬ ì¡°ê°ì˜ í•µì‹¬ ë¬¸ì¥ì„ ë½‘ì•„ ê°„ë‹¨íˆ ìš”ì•½í•œ ë‚´ìš©ì…ë‹ˆë‹¤. ë³´ë‹¤ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥/í•´ì„ì€ í´ë¼ìš°ë“œ LLM ì—°ë™ ì‹œ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.")
        bullet_points = []
        for idx in hits:
            text = meta["texts"][idx]
            for line in text.splitlines():
                line = line.strip()
                if len(line) > 0 and 10 <= len(line) <= 160 and line[-1] in ".):":
                    bullet_points.append(line)
                    break
        bullet_points = bullet_points[:5]
        if bullet_points:
            for bp in bullet_points:
                st.markdown(f"- {bp}")
        else:
            st.info("ëª…í™•í•œ ìš”ì•½ í¬ì¸íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´í™”í•´ë³´ì„¸ìš”.")

        st.caption("âš ï¸ ë²•ë¥ /ë³´í—˜ ë“± ì¤‘ìš”í•œ ê²°ì •ì€ ë°˜ë“œì‹œ ì›ë¬¸ ì•½ê´€ê³¼ ì „ë¬¸ê°€ ìƒë‹´ìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
